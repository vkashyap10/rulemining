{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-cf8086cdf0d8>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vishal/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_X = np.zeros((10000,2))\n",
    "data_y = np.zeros((10000,1))\n",
    "import random\n",
    "\n",
    "for i in range(data_X.shape[0]):\n",
    "    data_X[i,0] = random.uniform(0, 1)\n",
    "    data_X[i,1] = random.uniform(0, 1)\n",
    "    if(data_X[i,0] >= 0.5 and data_X[i,1]>=0.5):\n",
    "        data_y[i] = 1\n",
    "    else:\n",
    "        data_y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 200\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 1 #1st layer number of neurons\n",
    "n_hidden_2 = 15 #2nd layer number of neurons\n",
    "num_input = 2 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 1 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# initialize intervals for via\n",
    "# [0,1] since sigmoid is used\n",
    "input_interval = np.zeros((2,num_input))\n",
    "#input_interval[1,:] = input_interval[1,:] + 1\n",
    "input_interval[1,0] = 1\n",
    "input_interval[1,1] = 0.4\n",
    "\"\"\"\n",
    "input_interval = []\n",
    "for i in range(num_input):\n",
    "    input_interval.append([(0,255)])\n",
    "input_interval = np.array(input_interval)\n",
    "\"\"\"\n",
    "\n",
    "h1_interval = np.zeros((2,n_hidden_1))\n",
    "h1_interval[1,:] = h1_interval[1,:] + 1\n",
    "\"\"\"\n",
    "for i in range(n_hidden_1):\n",
    "    h1_interval.append([0,1])\n",
    "h1_interval = np.array(h1_interval)  \n",
    "\"\"\"\n",
    "h2_interval = np.zeros((2,n_hidden_2))\n",
    "h2_interval[1,:] = h2_interval[1,:] + 1\n",
    "\"\"\"\n",
    "h2_interval = []\n",
    "for i in range(n_hidden_2):\n",
    "    h2_interval.append([(0,1)])\n",
    "h2_interval = np.array(h2_interval)\n",
    "\"\"\"\n",
    "\n",
    "out_interval = np.zeros((2,num_classes))\n",
    "out_interval[1,:] = out_interval[1,:] + 1\n",
    "\"\"\"\n",
    "out_interval = []\n",
    "for i in range(num_classes):\n",
    "    out_interval.append([(0,1)])\n",
    "out_interval = np.array(out_interval)\n",
    "\"\"\"\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. ],\n",
       "       [1. , 0.4]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_2 = tf.sigmoid(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.sigmoid(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "loss = -(Y * tf.log(logits+ 1e-20) + (1 - Y) * tf.log( 1 - logits + 1e-20))\n",
    "loss_op = tf.reduce_mean(tf.reduce_sum(loss, reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "correct_pred = tf.equal(tf.greater_equal(logits, 0.5), tf.greater_equal(Y, 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 21, 10, 16]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(range(30), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    index = random.sample(range(data_X.shape[0]), batch_size)\n",
    "    return data_X[index],data_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.5954, Training Accuracy= 0.720\n",
      "Step 100, Minibatch Loss= 0.5186, Training Accuracy= 0.755\n",
      "Step 200, Minibatch Loss= 0.4430, Training Accuracy= 0.745\n",
      "Step 300, Minibatch Loss= 0.3155, Training Accuracy= 0.845\n",
      "Step 400, Minibatch Loss= 0.2879, Training Accuracy= 0.905\n",
      "Step 500, Minibatch Loss= 0.2594, Training Accuracy= 0.915\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9141\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    logit,f_accuracy,f_weights,f_biases = sess.run([logits,accuracy,weights,biases], feed_dict={X: data_X,Y: data_y})\n",
    "    print(\"Testing Accuracy:\", f_accuracy)\n",
    "    \n",
    "    print(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843534\n"
     ]
    }
   ],
   "source": [
    "print(np.max(logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    if(x>10):\n",
    "        return 1\n",
    "    if(x<-30):\n",
    "        return 1e-12\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "h1 = np.array(f_weights['h1'])\n",
    "#h2 = np.array(f_weights['h2'])\n",
    "out_w = np.array(f_weights['out'])\n",
    "b1 = np.array(f_biases['b1'])\n",
    "#b2 = np.array(f_biases['b2'])\n",
    "out_b = np.array(f_biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def objective(x,w):\n",
    "    w = np.array(w)\n",
    "    b = w[-1]\n",
    "    w = w[:-1]\n",
    "    k = sigmoid(np.sum(x*w) + b)\n",
    "    return np.sum(k)\n",
    "def objective_max(x,w):\n",
    "    w = np.array(w)\n",
    "    b = w[-1]\n",
    "    w = w[:-1]\n",
    "    k = sigmoid(np.sum(x*w) + b)\n",
    "    return -np.sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward phase for via\n",
    "def forward():\n",
    "    for i in range(h1.shape[1]):\n",
    "        w = h1[:,i]\n",
    "        #w = np.hstack((h1[:,i],b1[i]))\n",
    "        w = tuple(np.hstack((w,b1[i])))\n",
    "        bnds = []\n",
    "        for j in range(input_interval.shape[1]):\n",
    "            bnds.append((input_interval[0,j],input_interval[1,j]))\n",
    "        x0 = (0,0)\n",
    "        bnds = np.array(bnds)\n",
    "        bnds = tuple(bnds)\n",
    "        print(w)\n",
    "        sol = minimize (objective,x0,(w,),bounds = bnds)\n",
    "        h1_interval[0,i] = sol['fun']\n",
    "        sol = minimize(objective_max,x0,(w,),bounds = bnds)\n",
    "        h1_interval[1,i] = abs(sol['fun'])\n",
    "        \n",
    "        \n",
    "    # out layer\n",
    "    for i in range(out_w.shape[1]):\n",
    "        w = out_w[:,i]\n",
    "        #w = np.hstack((h1[:,i],b1[i]))\n",
    "        w = tuple(np.hstack((w,out_b[i])))\n",
    "        bnds = []\n",
    "        for j in range(h1_interval.shape[1]):\n",
    "            bnds.append((h1_interval[0,j],h1_interval[1,j]))\n",
    "        x0 = np.zeros((h1_interval.shape[1]))\n",
    "        bnds = np.array(bnds)\n",
    "        bnds = tuple(bnds)\n",
    "        sol = minimize (objective,x0,(w,),bounds = bnds)\n",
    "        out_interval[0,i] = sol['fun']\n",
    "        sol = minimize(objective_max,x0,(w,),bounds = bnds)\n",
    "        out_interval[1,i] = abs(sol['fun'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-4.101505, -4.224415, 4.6187196)\n"
     ]
    }
   ],
   "source": [
    "forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01891907],\n",
       "       [0.52693533]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(weights,interval,bias,mode):\n",
    "    A_ub = weights.T\n",
    "    ones = np.identity((weights.T).shape[0])\n",
    "    zeros = np.zeros(((weights.T).shape[0],(weights.T).shape[0]))\n",
    "    #A_ub = np.hstack((A_ub,ones,zeros))\n",
    "    # make intervals ready for log\n",
    "    for i in range(interval.shape[1]):\n",
    "        if(interval[1,i]==1):\n",
    "            interval[1,i]= 1-1e-6\n",
    "        if(interval[0,i]==0):\n",
    "            interval[0,i] = 1e-6\n",
    "    b_ub = -np.log(1/(interval[1,:]) - 1 ) - bias\n",
    "    b_ub1 = -np.log(1/(interval[0,:])-1 ) - bias\n",
    "    #A_ub1 = np.hstack((weights.T,zeros,ones))\n",
    "    A_ub = np.vstack((A_ub,-A_ub))\n",
    "    b_ub = np.hstack((b_ub,-b_ub1))\n",
    "    new_interval = np.zeros((A_ub.shape[1]))\n",
    "    bnds = []\n",
    "    print(\"solving\")\n",
    "    print(A_ub)\n",
    "    print(b_ub)\n",
    "    for j in range(A_ub.shape[1]):\n",
    "        bnds.append((0,1))\n",
    "    bnds = tuple(bnds)\n",
    "    for i in range(A_ub.shape[1]):\n",
    "        c = np.zeros((A_ub.shape[1]))\n",
    "        if(mode==\"min\"):\n",
    "            c[i] = 1\n",
    "        else:\n",
    "            c[i] = -1\n",
    "        res = scipy.optimize.linprog(c,A_ub = A_ub,b_ub = b_ub,options=dict(tol=1e-8),bounds = bnds)\n",
    "        #print(res)\n",
    "        new_interval[i] = res['x'][i]\n",
    "    return new_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def backward():\n",
    "    print(\"computing for hidden layer\")\n",
    "    max_bounds = bound(out_w,out_interval,out_b,\"max\")\n",
    "    h1_interval[1,:] = max_bounds\n",
    "    min_bounds = bound(out_w,out_interval,out_b,\"min\")\n",
    "    h1_interval[0,:] = min_bounds\n",
    "    print(\"new hidden interval\")\n",
    "    print(h1_interval)\n",
    "    #max_bounds = bound(h2,h2_interval,b2,\"max\")\n",
    "    #min_bounds = bound(h2,h2_interval,b2,\"min\")\n",
    "    print(\"computing for input interval\")\n",
    "    max_bounds = bound(h1,h1_interval,b1,\"max\")\n",
    "    input_interval[1,:] = max_bounds\n",
    "    min_bounds = bound(h1,h1_interval,b1,\"min\")\n",
    "    input_interval[0,:] = min_bounds\n",
    "    print(\"new input interval\")\n",
    "    print(input_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing for hidden layer\n",
      "solving\n",
      "[[-5.3809133]\n",
      " [ 5.3809133]]\n",
      "[-1.37986219 15.19537175]\n",
      "solving\n",
      "[[-5.3809133]\n",
      " [ 5.3809133]]\n",
      "[-1.37986219 15.19537175]\n",
      "new hidden interval\n",
      "[[0.25643643]\n",
      " [1.        ]]\n",
      "computing for input interval\n",
      "solving\n",
      "[[-4.101505 -4.224415]\n",
      " [ 4.101505  4.224415]]\n",
      "[9.19678998 5.68329304]\n",
      "solving\n",
      "[[-4.101505 -4.224415]\n",
      " [ 4.101505  4.224415]]\n",
      "[9.19678998 5.68329304]\n",
      "new input interval\n",
      "[[0. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_consistency(input_interval):\n",
    "    # initialize hidden layer/s and output intervals\n",
    "    h1_interval = np.zeros((2,n_hidden_1))\n",
    "    h1_interval[1,:] = h1_interval[1,:] + 1\n",
    "    out_interval = np.zeros((2,num_classes))\n",
    "    out_interval[1,:] = out_interval[1,:] + 1\n",
    "    abs_diff = 100\n",
    "    initial_val = np.sum(h1_interval) + np.sum(input_interval) + np.sum(out_interval)\n",
    "    while(abs_diff>0.5):\n",
    "        forward()\n",
    "        # check for consistency\n",
    "        if(out_interval[0,1]<0.5):\n",
    "            return 1\n",
    "        backward()\n",
    "        final_val = np.sum(h1_interval) + np.sum(input_interval) + np.sum(out_interval)\n",
    "        abs_diff = abs(initial_val - final_val)\n",
    "        initial_val = final_val\n",
    "    return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule extraction using via (and gate)\n",
    "input_max = 1\n",
    "input_min = 0\n",
    "input_interval = np.zeros((2,num_input))\n",
    "input_interval[1,0] = 1\n",
    "input_interval[1,1] = 0.4\n",
    "\n",
    "h1_interval = np.zeros((2,n_hidden_1))\n",
    "h1_interval[1,:] = h1_interval[1,:] + 1\n",
    "\n",
    "# set out interval according to rule to test (rules will be proved via contradiction using inconsistencies in VIA analysis)\n",
    "out_interval = np.zeros((2,num_classes))\n",
    "out_interval[1,:] = out_interval[1,:] + 1\n",
    "\n",
    "rules = []\n",
    "tol = 1e-1\n",
    "for i in range(data_X.shape[0]):\n",
    "    if(data_Y[i]==1):\n",
    "        flag = 0\n",
    "        for j in range(len(rules)):\n",
    "            if(flag ==1):\n",
    "                break\n",
    "            for k in range(data_X.shape[1]):\n",
    "                if(rules[j][0,k]>data_X[i,k] or rules[j][1,k]<data_X[i,k]):\n",
    "                    flag = 1\n",
    "                    break\n",
    "        if(flag==1):\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            for j in range(input_interval.shape[1]):\n",
    "                input_interval[0,j] = data_X[i,j]\n",
    "                input_interval[1,j] = data_X[i,j]\n",
    "            for k in range(input_interval.shape[1]):\n",
    "                # try decreasing the lower bound\n",
    "                test_val = 0\n",
    "                while(!test_val):\n",
    "                    test_interval = np.copy(input_interval)\n",
    "                    test_interval[0,k] = test_interval[0,k] - tol\n",
    "                    if(test_interval[0,k]<=0):\n",
    "                        break\n",
    "                    test_val = test_consistency(test_interval)\n",
    "                    if(!test_val):\n",
    "                        input_interval = np.copy(test_interval)\n",
    "                \n",
    "                #now try increasing the upper bound\n",
    "                test_val = 0\n",
    "                while(!test_val):\n",
    "                    test_interval = np.copy(input_interval)\n",
    "                    test_interval[1,k] = test_interval[1,k] + tol\n",
    "                    if(test_interval[1,k]>=1):\n",
    "                        break\n",
    "                    test_val = test_consistency(test_interval)\n",
    "                    if(!test_val):\n",
    "                        input_interval = np.copy(test_interval)\n",
    "            rules.append(input_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
